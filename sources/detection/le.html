
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Learning Entropy (LE) &#8212; Padasip 1.2.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Miscellaneous" href="../misc.html" />
    <link rel="prev" title="Extreme Seeking Entropy (ESE)" href="ese.html" />
    <meta name="description" content="Padasip - Python Adaptive Signal Processing">
    <meta name="keywords" content="Adaptive,Signal,Processing,Filters,Neural Networks,">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-73796119-2', 'auto');
      ga('send', 'pageview');
    </script>

  </head><body>


  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-padasip.detection.le">
<span id="learning-entropy-le"></span><span id="detection-le"></span><h1>Learning Entropy (LE)<a class="headerlink" href="#module-padasip.detection.le" title="Permalink to this headline">¶</a></h1>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.0.0.</span></p>
</div>
<p>The Learning Entropy (LE) is non-Shannon entropy based on conformity
of individual data samples to the contemporary learned governing law
of a learning system.</p>
<p>Content of this page:</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#algorithm-explanation" id="id1">Algorithm Explanation</a></p></li>
<li><p><a class="reference internal" href="#usage-instructions-and-optimal-performance" id="id2">Usage Instructions and Optimal Performance</a></p></li>
<li><p><a class="reference internal" href="#minimal-working-example" id="id3">Minimal Working Example</a></p></li>
<li><p><a class="reference internal" href="#code-explanation" id="id4">Code Explanation</a></p></li>
</ul>
</div>
<section id="algorithm-explanation">
<h2><a class="toc-backref" href="#id1">Algorithm Explanation</a><a class="headerlink" href="#algorithm-explanation" title="Permalink to this headline">¶</a></h2>
<p>Two options how to estimate the LE are implemented - direct approach and
multiscale approach.</p>
<p class="rubric">Direct approach</p>
<p>With direct approach the LE is evaluated for every sample as follows</p>
<p><span class="math notranslate nohighlight">\(\textrm{LE}_d(k) = \frac{ (\Delta \textbf{w}(k) -
\overline{| \Delta \textbf{w}_M(k) |}) }
{ (\sigma({| \Delta \textbf{w}_M(k) |})+\epsilon) }\)</span></p>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(|\Delta \textbf{w}(k)|\)</span> are the absolute values of current weights
increment.</p></li>
<li><p><span class="math notranslate nohighlight">\(\overline{| \Delta \textbf{w}_M(k) |}\)</span> are averages of absolute
values of window used for LE evaluation.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma (| \Delta \textbf{w}_M(k) |)\)</span> are standard deviatons of
absolute values of window used for LE evaluation.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is regularization term to preserve stability for small
values of standard deviation.</p></li>
</ul>
<p class="rubric">Multiscale approach</p>
<p>Value for every sample is defined as follows</p>
<p><span class="math notranslate nohighlight">\(\textrm{LE}(k) = \frac{1}{n \cdot n_\alpha}
\sum f(\Delta w_{i}(k), \alpha ),\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\Delta w_i(k)\)</span> stands for one weight from vector
<span class="math notranslate nohighlight">\(\Delta \textbf{w}(k)\)</span>, the <span class="math notranslate nohighlight">\(n\)</span> is number of weights,
the <span class="math notranslate nohighlight">\(n_\alpha\)</span> is number of used detection sensitivities</p>
<p><span class="math notranslate nohighlight">\(\alpha=[\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n_{\alpha}}].\)</span></p>
<p>The function <span class="math notranslate nohighlight">\(f(\Delta w_{i}(k), \alpha)\)</span> is defined as follows</p>
<p><span class="math notranslate nohighlight">\(f(\Delta w_{i}(k),\alpha)=
\{{\rm if}\,\left(\left\vert \Delta w_{i}(k)\right\vert &gt;
\alpha\cdot \overline{\left\vert \Delta w_{Mi}(k)\right\vert }\right)\,
\rm{then} \, 1, \rm{else  }\,0 \}.\)</span></p>
</section>
<section id="usage-instructions-and-optimal-performance">
<h2><a class="toc-backref" href="#id2">Usage Instructions and Optimal Performance</a><a class="headerlink" href="#usage-instructions-and-optimal-performance" title="Permalink to this headline">¶</a></h2>
<p>The LE algorithm can be used as follows</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">le</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">detection</span><span class="o">.</span><span class="n">learning_entropy</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>in case of direct approach. For multiscale approach an example follows</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">le</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">detection</span><span class="o">.</span><span class="n">learning_entropy</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">[</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">,</span> <span class="mf">13.</span><span class="p">])</span>
</pre></div>
</div>
<p>where <cite>w</cite> is matrix of the adaptive parameters (changing in time, every row
should represent one time index), <cite>m</cite> is window size, <cite>order</cite> is LE order  and
<cite>alpha</cite> is vector of sensitivities.</p>
<p class="rubric">Used adaptive models</p>
<p>In general it is possible to use any adaptive model. The input of the LE
algorithm is matrix of an adaptive parameters history, where every row
represents the parameters used in a particular time and every column represents
one parameter in whole adaptation history.</p>
<p class="rubric">Selection of sensitivities</p>
<p>The optimal number of detection sensitivities and their values
depends on task and data. The sensitivities should be chosen in range
where the function <span class="math notranslate nohighlight">\(LE(k)\)</span> returns a value lower than 1 for at
least one sample in data, and for at maximally one sample returns value of 0.</p>
</section>
<section id="minimal-working-example">
<h2><a class="toc-backref" href="#id3">Minimal Working Example</a><a class="headerlink" href="#minimal-working-example" title="Permalink to this headline">¶</a></h2>
<p>In this example is demonstrated how can the multiscale approach LE highligh
the position of a perturbation inserted in a data. As the adaptive model is used
<a class="reference internal" href="../filters/nlms.html#filter-nlms"><span class="std std-ref">Normalized Least-mean-squares (NLMS)</span></a> adaptive filter. The perturbation is manually inserted
in sample with index <span class="math notranslate nohighlight">\(k=1000\)</span> (the length of data is 2000).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">padasip</span> <span class="k">as</span> <span class="nn">pa</span>

<span class="c1"># data creation</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="c1"># perturbation insertion</span>
<span class="n">d</span><span class="p">[</span><span class="mi">1000</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">2.</span>

<span class="c1"># creation of learning model (adaptive filter)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">FilterNLMS</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
<span class="n">y</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># estimation of LE with weights from learning model</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">detection</span><span class="o">.</span><span class="n">learning_entropy</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="p">[</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">,</span> <span class="mf">13.</span><span class="p">])</span>

<span class="c1"># LE plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">le</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="code-explanation">
<h2><a class="toc-backref" href="#id4">Code Explanation</a><a class="headerlink" href="#code-explanation" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="padasip.detection.le.learning_entropy">
<span class="sig-prename descclassname"><span class="pre">padasip.detection.le.</span></span><span class="sig-name descname"><span class="pre">learning_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">w</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">order</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/padasip/detection/le.html#learning_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#padasip.detection.le.learning_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>This function estimates Learning Entropy.</p>
<p><strong>Args:</strong></p>
<ul class="simple">
<li><p><cite>w</cite> : history of adaptive parameters of an adaptive model (2d array),
every row represents parameters in given time index.</p></li>
</ul>
<p><strong>Kwargs:</strong></p>
<ul class="simple">
<li><p><cite>m</cite> : window size (1d array) - how many last samples are used for
evaluation of every sample.</p></li>
<li><p><cite>order</cite> : order of the LE (int) - order of weights differention</p></li>
<li><p><cite>alpha</cite> : list of senstitivites (1d array). If not provided, the LE
direct approach is used.</p></li>
</ul>
<p><strong>Returns:</strong></p>
<ul class="simple">
<li><p>Learning Entropy of data (1 d array) - one value for every sample</p></li>
</ul>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">

<a href="https://github.com/matousc89/padasip"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

<iframe src="https://ghbtns.com/github-btn.html?user=matousc89&repo=padasip&type=watch&count=true&size=large&v=2" frameborder="0" scrolling="0" width="160px" height="30px" style="margin-bottom: 5px;"></iframe>

<iframe src="https://ghbtns.com/github-btn.html?user=matousc89&repo=padasip&type=star&count=true&size=large" frameborder="0" scrolling="0" width="160px" height="30px" style="margin-bottom: 15px;"></iframe>

<h3><a href="../../index.html">Table of Contents</a></h3>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">Padasip</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../index.html#license">License</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#instalation">Instalation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#tutorials">Tutorials</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html#the-user-quide">The User Quide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#contact">Contact</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#changelog">Changelog</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../index.html#indices-and-tables">Indices and tables</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../preprocess.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../filters.html">Adaptive Filters</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../detection.html">Detection Tools</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="elbnd.html">Error and Learning Based Novelty Detection (ELBND)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ese.html">Extreme Seeking Entropy (ESE)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Learning Entropy (LE)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../misc.html">Miscellaneous</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../detection.html">Detection Tools</a><ul>
      <li>Previous: <a href="ese.html" title="previous chapter">Extreme Seeking Entropy (ESE)</a></li>
      <li>Next: <a href="../misc.html" title="next chapter">Miscellaneous</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Matous C.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.4.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/sources/detection/le.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>